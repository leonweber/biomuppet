{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DSm8ui9tI39"
      },
      "source": [
        "# Notebook for all tokcls (EBM PICO, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Prerequisites\n",
        "\n",
        "+ Open the notebooks for [seqcls datasets](https://colab.research.google.com/drive/18En1Y-OB6_sv3aik7L7iLMN7k41uExbu?usp=sharing) (PubMedQA, BioASQ, Biosses, hoc, ChemProt, DDI, GAD) or [tokcls datasets](https://colab.research.google.com/drive/1XTa3NJtQ5lkveec3WaniJM-0we6gZxtl?usp=sharing) (ebmnlp, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem)\n",
        "+ Copy the notebook to your drive (File -> Copy to Drive)\n",
        "\n",
        "### Instructions\n",
        "\n",
        "**Note: The only cell which requires editing should be the \"Configuration\" cell.**\n",
        "\n",
        "The notebook contains 4 major segments:\n",
        "\n",
        "1. Preparation: Contains pretty self-explanatory cells \"Download BLURB data\", \"Install libraries\" and \"Import dependencies, class definitions, static settings\". Should be run only once in the best case.\n",
        "2. Configuration: This cell should be the only one that should be edited. The most important variables are:\n",
        "\n",
        "  * `dataset_name`: should be one of `[PubMedQA, BioASQ, Biosses, hoc, ChemProt, DDI, GAD]` for seqcls or one of `[ebmnlp, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem]` for tokcls\n",
        "  * `max_train_epochs`: The maximum number of training epochs.\n",
        "  * `n_trials`: The number of trials in the hyperparameter search\n",
        "  * `hp_space_ray`: The hyperparameter search space\n",
        "3. Train with hyperparameter optimization: Runs the hyperparameter optimization with the configuration specified by `n_trials` and `hp_space_ray` in the configuration. The experiment should be located in `/runs/{dataset_name}/ray_results/{dataset_name}` where `dataset_name` is the dataset name that you specified.\n",
        "4. Load and evaluate best model from hyperparameter optimization: selects the best checkpoint from the experiment folder and evaluates the test set."
      ],
      "metadata": {
        "id": "IV_OLblVX2HW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnSgrXg9dBFa"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31vu0gY6SxU"
      },
      "source": [
        "### Download BLURB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22IX9SdKrhU3"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYWaY4Aw6Wde"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mLo-nowroW9"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers datasets fairscale sklearn seqeval\n",
        "!pip install ray psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcciYtpq6adU"
      },
      "source": [
        "### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HUZMitDwgzm"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import psutil\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import ClassLabel, load_dataset, load_metric\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import JupyterNotebookReporter\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizerFast,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "\"\"\"\n",
        "!pip install wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"f419b5da75121c5feb2c141a08733d99f8171dbd\"\n",
        "import wandb\n",
        "wandb.init(project=\"my-test-project\", entity=\"nomisto\")\n",
        "\"\"\"\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"-1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gvweJclwHaJ"
      },
      "source": [
        "## Configuration\n",
        "The following has to be configured for each dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmk7wJFF00C3"
      },
      "outputs": [],
      "source": [
        "# General model/dataset configuration\n",
        "\n",
        "## Name of the model to load, shouldn't be changed\n",
        "model_name = \"leonweber/bunsen_base_last\" \n",
        "\n",
        "## Name of the dataset to finetune, one of [ebmnlp, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem]\n",
        "dataset_name = \"JNLPBA\" \n",
        "\n",
        "## Automatically generates paths, should not be changed\n",
        "data_files = {\n",
        "  \"train\": f\"data/tokcls/{dataset_name}_hf/train.json\", \n",
        "  \"validation\": f\"data/tokcls/{dataset_name}_hf/dev.json\", \n",
        "  \"test\": f\"data/tokcls/{dataset_name}_hf/test.json\"\n",
        "}\n",
        "\n",
        "## Always False, should not be changed\n",
        "pad_to_max_length = False\n",
        "## Should not be changed\n",
        "max_seq_length = 512\n",
        "\n",
        "## The number of maximum training epochs\n",
        "## Every epoch a checkpoint is created, after training the one with the highest `metric_for_best_model` is selected.\n",
        "max_train_epochs=30\n",
        "\n",
        "## Metric to determine the best model\n",
        "## Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\"\n",
        "metric_for_best_model = \"f1\" if dataset_name != \"ebmnlp\" else \"macro_f1\"\n",
        "\n",
        "training_args = TrainingArguments( # huggingface training arguments https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments\n",
        "        output_dir=f\"./runs/{dataset_name}\",\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=5,\n",
        "        save_total_limit=1,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_dir=\"./logs\",\n",
        "        skip_memory_metrics=True,\n",
        "        report_to=\"none\",\n",
        "        logging_steps=100, # logging steps for train loss\n",
        "        do_predict=True,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=metric_for_best_model,\n",
        "        greater_is_better=True,\n",
        "        gradient_accumulation_steps=1,\n",
        "        fp16=True,\n",
        "        num_train_epochs=max_train_epochs\n",
        "    )\n",
        "\n",
        "\n",
        "# Hyperparameter optimization (HPO)\n",
        "\n",
        "## maximize if metric is \"bigger is better\", else: minimize\n",
        "direction=\"maximize\" # maximize if metric is bigger_is_better, else: minimize, should not be changed\n",
        "\n",
        "## Number of different trials for HPO\n",
        "## Note that if you have grid_search (e.g. tune.choice([4, 8, 16])) in the config below, `n_trials` is how many\n",
        "## trials (samples of random variables) are tried for each distinct combination\n",
        "## of grid search. F.e. n_trails=2 and the following config\n",
        "##\n",
        "## hp_space_ray(trial): \n",
        "##    return {\n",
        "##        \"per_device_train_batch_size\": tune.grid_search([4, 8, 16]),\n",
        "##        \"num_train_epochs\": tune.grid_search([5,10,20]),\n",
        "##        \"learning_rate\": tune.loguniform(1e-5, 5e-5),\n",
        "##    }\n",
        "##\n",
        "## would result in a total of 18 trials.\n",
        "## see https://docs.ray.io/en/latest/tune/api_docs/search_space.html#\n",
        "n_trials = 10 \n",
        "\n",
        "## Hyperparameter search space, \n",
        "## These overwrite training_args above\n",
        "## For forther information see see https://docs.ray.io/en/latest/tune/key-concepts.html#search-spaces\n",
        "## This should not contain configurations for the parameter 'num_train_epochs' see variable 'max_train_epochs' above\n",
        "def hp_space_ray(trial): \n",
        "    return {\n",
        "        \"learning_rate\": tune.loguniform(1e-5, 5e-5),\n",
        "        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-be15qz6FWM"
      },
      "source": [
        "# Train with hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hltyIzHhxWP0"
      },
      "outputs": [],
      "source": [
        "### Seeding\n",
        "set_seed(training_args.seed) # Set seed before initializing model.\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")\n",
        "\n",
        "log_level = training_args.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "# Log on each process the small summary:\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")\n",
        "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "# Loading a dataset from your local files.\n",
        "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "column_names = raw_datasets[\"train\"].column_names\n",
        "features = raw_datasets[\"train\"].features\n",
        "\n",
        "text_column_name = \"tokens\"\n",
        "label_column_name = \"ner_tags\"\n",
        "\n",
        "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
        "# unique labels.\n",
        "def get_label_list(labels):\n",
        "    unique_labels = set()\n",
        "    for label in labels:\n",
        "        unique_labels = unique_labels | set(label)\n",
        "    label_list = list(unique_labels)\n",
        "    label_list.sort()\n",
        "    return label_list\n",
        "\n",
        "if isinstance(features[label_column_name].feature, ClassLabel):\n",
        "    label_list = features[label_column_name].feature.names\n",
        "    # No need to convert the labels since they are already ints.\n",
        "    label_to_id = {i: i for i in range(len(label_list))}\n",
        "else:\n",
        "    label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
        "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "num_labels = len(label_list)\n",
        "\n",
        "# Initialize model, tokenizer, config\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label_to_id,\n",
        "    id2label={i: l for l, i in label_to_id.items()},\n",
        "    finetuning_task=\"ner\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "## Needs to be encapsulated for hpo\n",
        "def model_init():\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        config=config\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Preprocessing of dataset\n",
        "# Padding strategy\n",
        "padding = \"max_length\" if pad_to_max_length else False\n",
        "\n",
        "# Tokenize all texts and align the labels with them.\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[text_column_name],\n",
        "        padding=padding,\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
        "        is_split_into_words=True,\n",
        "    )\n",
        "    labels = []\n",
        "    word_ids_list = []\n",
        "    for i, label in enumerate(examples[label_column_name]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        word_ids_list.append(word_ids)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label_to_id[label[word_idx]])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    tokenized_inputs[\"word_ids\"] = word_ids_list\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
        "    train_dataset = train_dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        desc=\"Running tokenizer on train dataset\",\n",
        "    )\n",
        "\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
        "    eval_dataset = eval_dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        desc=\"Running tokenizer on validation dataset\",\n",
        "    )\n",
        "\n",
        "predict_dataset = raw_datasets[\"test\"]\n",
        "with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
        "    predict_dataset = predict_dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        desc=\"Running tokenizer on prediction dataset\",\n",
        "    )\n",
        "\n",
        "## Init trainer\n",
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    if dataset_name == \"ebmnlp\":\n",
        "        Ps, Rs, Fs = [], [], []\n",
        "        for type_name in results:\n",
        "            if type_name.startswith(\"overall\"):\n",
        "                continue\n",
        "            print ('type_name', type_name)\n",
        "            Ps.append(results[type_name][\"precision\"])\n",
        "            Rs.append(results[type_name][\"recall\"])\n",
        "            Fs.append(results[type_name][\"f1\"])\n",
        "        return {\n",
        "            \"macro_precision\": np.mean(Ps),\n",
        "            \"macro_recall\": np.mean(Rs),\n",
        "            \"macro_f1\": np.mean(Fs),\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "## needed only for google colab\n",
        "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
        "\n",
        "metric_columns = [\"epoch\", \"training_iteration\", \"objective\", \"eval_loss\"]\n",
        "if dataset_name == \"ebmnlp\":\n",
        "  metric_columns.extend([\"eval_macro_precision\", \"eval_macro_recall\", \"eval_macro_f1\"])\n",
        "else:\n",
        "  metric_columns.extend([\"eval_precision\", \"eval_recall\", \"eval_f1\", \"eval_accuracy\"])\n",
        "\n",
        "reporter = JupyterNotebookReporter(\n",
        "    False,\n",
        "    metric_columns=metric_columns,\n",
        ")\n",
        "\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "  direction=direction,\n",
        "  backend=\"ray\",\n",
        "  hp_space=hp_space_ray,\n",
        "  keep_checkpoints_num=1,\n",
        "  n_trials=n_trials,\n",
        "  local_dir=f\"./runs/{dataset_name}/ray_results/\",\n",
        "  name=dataset_name,\n",
        "  progress_reporter=reporter,\n",
        "  resources_per_trial={\n",
        "      \"cpu\": 1,\n",
        "      \"gpu\": 1\n",
        "  },\n",
        "  compute_objective = lambda x: x[\"eval_\" + metric_for_best_model]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytKxOvhwZfxB"
      },
      "source": [
        "# Load and evaluate best model from HPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvSuAGBGZgIG"
      },
      "outputs": [],
      "source": [
        "def recover_checkpoint(tune_checkpoint_dir, model_name=None):\n",
        "    if tune_checkpoint_dir is None or len(tune_checkpoint_dir) == 0:\n",
        "        return model_name\n",
        "    # Get subdirectory used for Huggingface.\n",
        "    subdirs = [\n",
        "        os.path.join(tune_checkpoint_dir, name)\n",
        "        for name in os.listdir(tune_checkpoint_dir)\n",
        "        if os.path.isdir(os.path.join(tune_checkpoint_dir, name))\n",
        "    ]\n",
        "    # There should only be 1 subdir.\n",
        "    assert len(subdirs) == 1, subdirs\n",
        "    return subdirs[0]\n",
        "\n",
        "ray_result_dir = f\"./runs/{dataset_name}/ray_results/{dataset_name}\"\n",
        "\n",
        "from ray.tune import ExperimentAnalysis\n",
        "analysis = ExperimentAnalysis(ray_result_dir)\n",
        "best_checkpoint = recover_checkpoint(\n",
        "    analysis.get_best_checkpoint(analysis.get_best_trial(metric=\"objective\",\n",
        "                            mode=\"max\" if direction==\"maximize\" else \"min\"), metric=\"objective\",\n",
        "                            mode=\"max\" if direction==\"maximize\" else \"min\").local_path\n",
        ")\n",
        "best_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    best_checkpoint)\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=best_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "metrics[\"eval_samples\"] = len(eval_dataset)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "results = trainer.predict(predict_dataset, metric_key_prefix=\"test\")\n",
        "predictions = results.predictions\n",
        "metrics = results.metrics\n",
        "metrics[\"test_samples\"] = len(predict_dataset)\n",
        "\n",
        "trainer.log_metrics(\"test\", metrics)\n",
        "trainer.save_metrics(\"test\", metrics)\n",
        "trainer.log(metrics)\n",
        "\n",
        "import json\n",
        "output_dir = training_args.output_dir\n",
        "output_path = f\"{output_dir}/test_outputs.json\"\n",
        "json.dump({\"predictions\": results.predictions.tolist(), \"label_ids\": results.label_ids.tolist()},\n",
        "              open(output_path, \"w\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "run_blurb_tokcls.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
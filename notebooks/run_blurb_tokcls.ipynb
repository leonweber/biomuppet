{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DSm8ui9tI39"
   },
   "source": [
    "# Notebook for all tokcls (EBM PICO, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnSgrXg9dBFa"
   },
   "source": [
    "## Download BLURB, install and import libs, class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E31vu0gY6SxU"
   },
   "source": [
    "### Download BLURB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22IX9SdKrhU3",
    "outputId": "394a7afa-a438-4da7-c9a0-a524eaf39785"
   },
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYWaY4Aw6Wde"
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mLo-nowroW9",
    "outputId": "82d36740-a226-46a2-a8c7-366c41f43086"
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install transformers==4.9.1 datasets==1.11.0 fairscale==0.4.0 wandb sklearn seqeval\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVe5J68lsq-j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "!pip install wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"f419b5da75121c5feb2c141a08733d99f8171dbd\"\n",
    "import wandb\n",
    "wandb.init(project=\"my-test-project\", entity=\"nomisto\")\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcciYtpq6adU"
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HUZMitDwgzm"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import ClassLabel, load_dataset, load_metric\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gvweJclwHaJ"
   },
   "source": [
    "## Configuration\n",
    "The following has to be configured for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainargs = {\n",
    "    \"ebmnlp\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_train_epochs\": 1,\n",
    "    },\n",
    "    \"JNLPBA\": {\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"num_train_epochs\": 5,\n",
    "    },\n",
    "    \"NCBI-disease\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"num_train_epochs\": 20,\n",
    "    },\n",
    "    \"BC2GM\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 6e-5,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"num_train_epochs\": 50,\n",
    "    },\n",
    "    \"BC5CDR-disease\": {\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"num_train_epochs\": 8,\n",
    "    },\n",
    "    \"BC5CDR-chem\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"num_train_epochs\": 20,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmk7wJFF00C3",
    "outputId": "5fc5fcf9-9785-45e7-b91c-fbcef89ba313"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"JNLPBA\" # one of [ebmnlp, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem]\n",
    "\n",
    "data_files = {\n",
    "  \"train\": f\"data/tokcls/{dataset_name}_hf/train.json\", \n",
    "  \"validation\": f\"data/tokcls/{dataset_name}_hf/dev.json\", \n",
    "  \"test\": f\"data/tokcls/{dataset_name}_hf/test.json\"\n",
    "}\n",
    "\n",
    "pad_to_max_length = False\n",
    "max_seq_length = 512\n",
    "\n",
    "training_args = TrainingArguments( # huggingface training arguments https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments\n",
    "        output_dir=f\"./runs/{dataset_name}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        max_steps=-1,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        skip_memory_metrics=True,\n",
    "        report_to=\"none\" if os.environ[\"WANDB_DISABLED\"] == \"true\" else \"wandb\",\n",
    "        logging_steps=100, # logging steps for train loss\n",
    "        do_predict=True,\n",
    "        load_best_model_at_end=False, # do we do this here, linkbert doesn't, would require metric_for_best_model and greater_is_better\n",
    "        **trainargs.get(dataset_name)\n",
    "    )\n",
    "\n",
    "\n",
    "### HPO \n",
    "direction=\"maximize\" # maximize if metric is bigger_is_better, else: minimize\n",
    "n_trials = 10 # Number of trials for HPO\n",
    "def hp_space_ray(trial): # Hyperparameter search space, overwriting training_args\n",
    "    return {\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"num_train_epochs\": tune.choice(range(1, 6)),\n",
    "        #\"seed\": tune.choice(range(1, 41)), check with set_seed above, needed anyway?\n",
    "        \"per_device_train_batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "    }\n",
    "\n",
    "### Seeding\n",
    "set_seed(training_args.seed) # Set seed before initializing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uneG1gr28N2y"
   },
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hltyIzHhxWP0",
    "outputId": "b5e87d8c-08ce-4e8a-f991-dfaf9aa04a94"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLBdgg2KajJ9"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "- Load raw dataset from json files\n",
    "- Set `is_regression` (BIOSSES) and `is_multiclass_binary` (HOC)\n",
    "- Create list of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330,
     "referenced_widgets": [
      "47e426f8c64442c39045c7726bd79056",
      "38ca45e3e2384862a1d60bd7a7ba9ae8",
      "ee810bc8ecc0491a858ba5687afd4fca",
      "e1719f21ef94454fb9406ea001f93eee",
      "66ff81cc6ac3464bbb6c57d1eb9a389a",
      "ae6063e8af1046a59065fdd5694c1fb6",
      "6d9ef70ead6a453ead711f26416e7b1a",
      "ea9a04cbfe744477b697b72fb93d5d1c",
      "a0a0429479bc4773b685b2a3070dd615",
      "677158d7947d424f8474a8093a787ecd",
      "9309f6fd59104ef0864a7969dc3f74a4",
      "f3ed8cece13449ca8aae63aacd0405ff",
      "37d1168795644443a6d43564d789f7d1",
      "c6924c8cb15c4f25adfe782fbf717179",
      "64f0f320319a4ef7b0e8d5b95da7f404",
      "8b97b50082884a8ea69b994b2d2b9a48",
      "1cfa2be9058a47aba694b6191a1f116d",
      "1bec15dc8272455cb2cbe3cc9e19cbec",
      "4ca6ec5f6b284858911f0da6308c5781",
      "21aee12552be48aa995a865e4263e6f7",
      "5d7f82d3c1974c0dba232c0598b27e9d",
      "7e17afb87b584633a573c366979f454b",
      "f23089a26c22408fae085fced596d246",
      "4ee6dbb3fc1d42fc8f1d55701e12bbac",
      "9c84543ba0ef41d6953a12475d89abe5",
      "46fb50a2e1f0421d9f4c16da7ef7ab57",
      "bf2ff8ae6ff64bcdbad1ca8322b9b81d",
      "259d87c1341543d691338a9857cb08e6",
      "b7a5b522efcc4bca87d33d5b02502223",
      "c0b2ba0a747c4cf7bb0e66bd63ecc06f",
      "48afee19459c46c9bc3313b39b53b9ff",
      "f317433e543d40408f4c4110b58244b7",
      "655b0046e8154d8dad9f97428137b0cf",
      "1f7d9157613749838d9ff6d3aa87614b",
      "f7219d16123f40e0901ae33e2d2f2225",
      "54caf333fef04cbf98675dd0528aca6b",
      "38b3b8ebfe9549d4973958d42a1f3dcd",
      "b7c5f5622d994b6ebb94de35aafd3434",
      "08441f0f631b496b9761da4e160b76f4",
      "17e181b4da0745609d10e6ea297003c7",
      "2a6af664a52144d6a5fbd2237b044400",
      "aa7c9b5e858e47bf9bedd462e76ba3fa",
      "e9fb3b0a73d846f78cf9e4aa42b88f5b",
      "3a7c67b0e78b4f9f965767f1a05dd8d1",
      "dee107db791449a1b9db1d19d4b8b4fe",
      "6514f9ecdd3c48e1bb83d7ae97a50796",
      "caf0c3a5dc1449a6a7469bc3d6b7dd3c",
      "4043d18aeb0a4f1598ff7c395b1c2f45",
      "e2131509833148e492c5ff0a4e54e96e",
      "fd8362b192274164bca35518d638bb7f",
      "0107db5f93e341899a347e30113e6b3d",
      "6e7fa171bf2249f9976b35a74bcdb224",
      "12756c817c4b49949ab473881470330c",
      "c04f45b7d1034810913b1e7932fe3406",
      "89a95665a7de4739b70c7b6ef8cca74a",
      "23a3a82c94564bb2b28bc9042f6204b4",
      "4ecf829f89484759bc96886e74e0c1d8",
      "c9740e176a234e6a81e8011f938d1178",
      "ae73375db46c4b37ad4f31e906823579",
      "7276b1e58ff84c2e97e886b3fd7246de",
      "b09a817636734e86be1fc4415751211d",
      "172b6883da224e9db737b22b3d892192",
      "390237d153274b5e8c3f67b1872b9794",
      "91d6306767b44273af22518776b6475d",
      "069fd947e9524cd481dfac2ca096e411",
      "a68daad0336d4960843a4665004a059e"
     ]
    },
    "id": "TLmgcSCjainc",
    "outputId": "dbc26bf4-8793-4246-ed8c-79afe2746560"
   },
   "outputs": [],
   "source": [
    "# Loading a dataset from your local files.\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "features = raw_datasets[\"train\"].features\n",
    "\n",
    "text_column_name = \"tokens\"\n",
    "label_column_name = \"ner_tags\"\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "if isinstance(features[label_column_name].feature, ClassLabel):\n",
    "    label_list = features[label_column_name].feature.names\n",
    "    # No need to convert the labels since they are already ints.\n",
    "    label_to_id = {i: i for i in range(len(label_list))}\n",
    "else:\n",
    "    label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjaKr2-I-Jq1"
   },
   "source": [
    "## Initialize model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqxxjoTxx63J",
    "outputId": "71e60cf0-2317-437c-e0d0-0513b6a3672a"
   },
   "outputs": [],
   "source": [
    "model_name = \"michiyasunaga/BioLinkBERT-base\"\n",
    "# model_name = \"sshleifer/tiny-distilroberta-base\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label_to_id,\n",
    "    id2label={i: l for l, i in label_to_id.items()},\n",
    "    finetuning_task=\"ner\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "## Needs to be encapsulated for hpo\n",
    "def model_init():\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WTSBirP71km"
   },
   "source": [
    "## Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "referenced_widgets": [
      "f6b2e2a7245e43da8eac4cf63b08bef0",
      "a1ae6d910ee844049835bd58e1057e9c",
      "ddc4adb08680458c8599f9c6d47c61a5",
      "da086c9de3834d158090e2af78cf1ead",
      "47912cc72e784dd09c3ea9e78ab8bcbb",
      "591be5d601404a4caceb97e606ade270",
      "9cdb659b2d0743a7af65e5c390fde724",
      "7fa462bb24a84d219c1ed236b2086a45",
      "2fb48e0827df465692c544657f70e996",
      "cb8ef378270940f482757e381869ab25",
      "a5009e667f384040be444aa211756966",
      "3c8a228bbb3e47e4bf20e7ff471bd09a",
      "57c9c90e7bcb44e2812c9bf131b2107b",
      "5fd3c0a01bdd47ac87d3a6ffd4c83d2b",
      "ee7830f60e8545e490859d3b81a860da",
      "cd3790027a7340eeb89f71078b8da896",
      "8a8cc1630ad4437c983fdebbc20f3fbb",
      "e2c321c12d3d4301b152756aa5d3ea0d",
      "11f400b548564fc29fbc50756fb7b455",
      "e43908c0657a43fe9042f10003195c70",
      "98817415c10d449e855722a72f37a102",
      "066cfa49da724deab6678927181d7d51",
      "9f388322e4a549e98df180e540249565",
      "8470e01567f942f9bb26c289bceec931",
      "03ca0eaf688f4b94aa727f39b1a86b2c",
      "8e82e9fa4370481784c06f859ab3743f",
      "3132beb9cc774afea50ebba5bbcd929d",
      "a72cc52d3f5c4869b088aa06bf961fde",
      "10142dee477a4ee3b48f23b67168f380",
      "927368747d444d41bc320642637a1dbb",
      "9bfbad9d61d44e61a73211b49f07a803",
      "bd866efa34984a728eab3243516f123b",
      "0e75f6ecf654401ba7fbeed566f92ebd"
     ]
    },
    "id": "4Tw7Av3YQ2lI",
    "outputId": "930bbcfd-18d6-4880-d090-d483da1ba039"
   },
   "outputs": [],
   "source": [
    "# Padding strategy\n",
    "padding = \"max_length\" if pad_to_max_length else False\n",
    "\n",
    "# Tokenize all texts and align the labels with them.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    word_ids_list = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        word_ids_list.append(word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"word_ids\"] = word_ids_list\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "    train_dataset = train_dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on validation dataset\",\n",
    "    )\n",
    "\n",
    "predict_dataset = raw_datasets[\"test\"]\n",
    "with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
    "    predict_dataset = predict_dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvo24abKzNRR"
   },
   "source": [
    "## Init trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319,
     "referenced_widgets": [
      "539c52c581b94897a88f88ddb1ab2652",
      "f3088b7f6d224dbbbad082ffa7976080",
      "a01fe9eb8fc94427a8e4db1039cfc606",
      "aa303bdcc296400997ecacf8c46691d4",
      "7d6d5247ce0040b599930debdae6a7f0",
      "da0deb7ea96c45ac892f75cf5593f4c0",
      "4f1a7512f68c4f62b44293501e54d709",
      "73e916f6e68540fb8c6c4a3a1be9a626",
      "f52a27e384c64f68803ee67f4f43926e",
      "7661c2992d36494687310c5b21b31762",
      "aee77cb5c16341d6a28993d742b24462"
     ]
    },
    "id": "icEs6LWGzJXD",
    "outputId": "4c9a887a-49fb-4339-e684-dd6dab21d614"
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if dataset_name == \"ebmnlp\":\n",
    "        Ps, Rs, Fs = [], [], []\n",
    "        for type_name in results:\n",
    "            if type_name.startswith(\"overall\"):\n",
    "                continue\n",
    "            print ('type_name', type_name)\n",
    "            Ps.append(results[type_name][\"precision\"])\n",
    "            Rs.append(results[type_name][\"recall\"])\n",
    "            Fs.append(results[type_name][\"f1\"])\n",
    "        return {\n",
    "            \"macro_precision\": np.mean(Ps),\n",
    "            \"macro_recall\": np.mean(Rs),\n",
    "            \"macro_f1\": np.mean(Fs),\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-be15qz6FWM"
   },
   "source": [
    "# Train/Eval/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkMPnZzn6Bhg"
   },
   "source": [
    "## With HPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjCbxn7D3R29",
    "outputId": "e6bd17c8-79fa-4604-9b60-78db04131699"
   },
   "outputs": [],
   "source": [
    "## needed only for google colab\n",
    "# ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
    "\n",
    "reporter = JupyterNotebookReporter(False)\n",
    "\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "  direction=direction,\n",
    "  backend=\"ray\",\n",
    "  hp_space=hp_space_ray,\n",
    "  keep_checkpoints_num=1,\n",
    "  n_trials=n_trials,\n",
    "  local_dir=f\"./runs/{dataset_name}/ray_results/\",\n",
    "  name=dataset_name,\n",
    "  progress_reporter=reporter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytKxOvhwZfxB"
   },
   "source": [
    "### Load best model from HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvSuAGBGZgIG"
   },
   "outputs": [],
   "source": [
    "def recover_checkpoint(tune_checkpoint_dir, model_name=None):\n",
    "    if tune_checkpoint_dir is None or len(tune_checkpoint_dir) == 0:\n",
    "        return model_name\n",
    "    # Get subdirectory used for Huggingface.\n",
    "    subdirs = [\n",
    "        os.path.join(tune_checkpoint_dir, name)\n",
    "        for name in os.listdir(tune_checkpoint_dir)\n",
    "        if os.path.isdir(os.path.join(tune_checkpoint_dir, name))\n",
    "    ]\n",
    "    # There should only be 1 subdir.\n",
    "    assert len(subdirs) == 1, subdirs\n",
    "    return subdirs[0]\n",
    "\n",
    "ray_result_dir = f\"./runs/{dataset_name}/ray_results/{dataset_name}\"\n",
    "\n",
    "from ray.tune import ExperimentAnalysis\n",
    "analysis = ExperimentAnalysis(ray_result_dir)\n",
    "best_checkpoint = recover_checkpoint(\n",
    "    analysis.get_best_trial(metric=\"objective\",\n",
    "                            mode=\"max\" if direction==\"maximize\" else \"min\").checkpoint.value\n",
    ")\n",
    "best_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    best_checkpoint)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CSbl8_wZjl4"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "FmPnL-KYQX68",
    "outputId": "1028842f-7682-46e7-f838-b1f9a46c7f6d"
   },
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZaoPeMNZX2O"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "7aOt8cXCZXQN",
    "outputId": "47089b96-a97a-424a-a037-42ef5280c490"
   },
   "outputs": [],
   "source": [
    "results = trainer.predict(predict_dataset, metric_key_prefix=\"test\")\n",
    "predictions = results.predictions\n",
    "metrics = results.metrics\n",
    "metrics[\"test_samples\"] = len(predict_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)\n",
    "trainer.log(metrics)\n",
    "\n",
    "import json\n",
    "output_dir = training_args.output_dir\n",
    "output_path = f\"{output_dir}/test_outputs.json\"\n",
    "json.dump({\"predictions\": results.predictions.tolist(), \"label_ids\": results.label_ids.tolist()},\n",
    "              open(output_path, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hw8den-YahQ"
   },
   "source": [
    "## Simple (should not be used/only for reprocucing LinkBERT numbers)\n",
    "\n",
    "To reproduce exact numbers\n",
    "\n",
    "- do `model=model_init()` right after model_init function definition (in \"Initialize model, tokenizer, config\") and change in Trainer init `model_init=model_init` to `model=model` (Otherwise the rng is not the same with the original script)\n",
    "- Install same library versions as LinkBERT\n",
    "\n",
    "```\n",
    "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install transformers==4.9.1 datasets==1.11.0 fairscale==0.4.0 wandb sklearn seqeval\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7oouukJzAru"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "KJukRskTzBDl",
    "outputId": "51e9ca1a-d7a4-4dac-d54a-d14dbd73ac32"
   },
   "outputs": [],
   "source": [
    "logger.info(\"*** Train ***\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(train_dataset)\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg9dQFUGy-Lg"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "ToTiJ6nHy61B",
    "outputId": "3b05c4bb-60db-473c-9eca-7a694bc12a37"
   },
   "outputs": [],
   "source": [
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EywNFTMwy4gu"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "z6NHb4rNy2R-",
    "outputId": "094ce51a-5bb2-4ba0-9d1c-de04426b7291"
   },
   "outputs": [],
   "source": [
    "logger.info(\"*** Predict ***\")\n",
    "\n",
    "results = trainer.predict(predict_dataset, metric_key_prefix=\"test\")\n",
    "predictions = results.predictions\n",
    "metrics = results.metrics\n",
    "metrics[\"test_samples\"] = len(predict_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)\n",
    "trainer.log(metrics)\n",
    "\n",
    "import json\n",
    "output_dir = training_args.output_dir\n",
    "output_path = f\"{output_dir}/test_outputs.json\"\n",
    "json.dump({\"predictions\": results.predictions.tolist(), \"label_ids\": results.label_ids.tolist()},\n",
    "              open(output_path, \"w\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Run_tokcls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

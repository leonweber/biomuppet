{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DSm8ui9tI39"
   },
   "source": [
    "# Notebook for all seqcls (PubMedQA, BioASQ, BIOSSES, HOC, ChemProt, DDI, GAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnSgrXg9dBFa"
   },
   "source": [
    "## Download BLURB, install and import libs, class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E31vu0gY6SxU"
   },
   "source": [
    "### Download BLURB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22IX9SdKrhU3",
    "outputId": "ea26d846-2b06-422a-e44e-64b7c95fe8c0"
   },
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYWaY4Aw6Wde"
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mLo-nowroW9",
    "outputId": "82d36740-a226-46a2-a8c7-366c41f43086"
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install transformers==4.9.1 datasets==1.11.0 fairscale==0.4.0 wandb sklearn seqeval\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVe5J68lsq-j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "!pip install wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"f419b5da75121c5feb2c141a08733d99f8171dbd\"\n",
    "import wandb\n",
    "wandb.init(project=\"my-test-project\", entity=\"nomisto\")\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcciYtpq6adU"
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HUZMitDwgzm"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, is_torch_tpu_available\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVUavqkfc2xN"
   },
   "source": [
    "### Define SeqClsTrainer\n",
    "Copied straight from https://raw.githubusercontent.com/michiyasunaga/LinkBERT/main/src/seqcls/trainer_seqcls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0LRohsLtst4"
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "A subclass of `Trainer` specific to Question-Answering tasks\n",
    "\"\"\"\n",
    "\n",
    "class SeqClsTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.evaluation_loop\n",
    "        output = eval_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                prediction_loss_only=None,\n",
    "                ignore_keys=ignore_keys,\n",
    "        )\n",
    "        # self.label_names = label_names\n",
    "        self.compute_metrics = compute_metrics\n",
    "\n",
    "        # metrics = output.metrics\n",
    "        metrics = self.compute_metrics(output, eval_dataset)\n",
    "        metrics[f\"{metric_key_prefix}_loss\"] = output.metrics[f\"{metric_key_prefix}_loss\"]\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        self.log(metrics)\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, predict_dataset, ignore_keys=None, metric_key_prefix: str = \"test\"):\n",
    "        predict_dataloader = self.get_test_dataloader(predict_dataset)\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.evaluation_loop\n",
    "        output = eval_loop(\n",
    "            predict_dataloader,\n",
    "            description=\"Prediction\",\n",
    "            prediction_loss_only=None,\n",
    "            ignore_keys=ignore_keys,\n",
    "        )\n",
    "\n",
    "        # self.label_names = label_names\n",
    "        self.compute_metrics = compute_metrics\n",
    "\n",
    "        # metrics = output.metrics\n",
    "        metrics = self.compute_metrics(output, predict_dataset)\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        self.log(metrics) #Added\n",
    "\n",
    "        return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gvweJclwHaJ"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "997KqkjmgX-N"
   },
   "outputs": [],
   "source": [
    "### Paths and metric names of datasets\n",
    "datasets = {\n",
    "    \"PubMedQA\": {\n",
    "        \"metric_name\": \"accuracy\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/pubmedqa_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/pubmedqa_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/pubmedqa_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 512,\n",
    "    },\n",
    "    \"BioASQ\": {\n",
    "        \"metric_name\": \"accuracy\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/bioasq_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/bioasq_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/bioasq_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 512,\n",
    "    },\n",
    "    \"Biosses\": {\n",
    "        \"metric_name\": \"pearsonr\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/BIOSSES_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/BIOSSES_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/BIOSSES_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 512,\n",
    "    },\n",
    "    \"hoc\": {\n",
    "        \"metric_name\": \"hoc\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/HoC_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/HoC_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/HoC_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 512,\n",
    "    },\n",
    "    \"ChemProt\": {\n",
    "        \"metric_name\": \"PRF1\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/chemprot_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/chemprot_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/chemprot_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 256,\n",
    "    },\n",
    "    \"DDI\": {\n",
    "        \"metric_name\": \"PRF1\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/DDI_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/DDI_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/DDI_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 256,\n",
    "    },\n",
    "    \"GAD\": {\n",
    "        \"metric_name\": \"PRF1\",\n",
    "        \"data_files\": {\n",
    "          \"train\": \"data/seqcls/GAD_hf/train.json\", \n",
    "          \"validation\": \"data/seqcls/GAD_hf/dev.json\", \n",
    "          \"test\": \"data/seqcls/GAD_hf/test.json\"\n",
    "        },\n",
    "        \"max_seq_length\": 256,\n",
    "    },\n",
    "}\n",
    "\n",
    "### LinkBERT hyperparameters\n",
    "trainargs = {\n",
    "    \"PubMedQA\": {\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"num_train_epochs\": 30,\n",
    "    },\n",
    "    \"BioASQ\": {\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"num_train_epochs\": 20,\n",
    "    },\n",
    "    \"Biosses\": {\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"num_train_epochs\": 30,\n",
    "        \"seed\": 5\n",
    "    },\n",
    "    \"hoc\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"num_train_epochs\": 40,\n",
    "    },\n",
    "    \"ChemProt\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "    \"DDI\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_train_epochs\": 5,\n",
    "    },\n",
    "    \"GAD\": {\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"fp16\": True,\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljWkGL1Mgcza"
   },
   "source": [
    "### The following has to be configured for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmk7wJFF00C3"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"BioASQ\"\n",
    "metric_name, data_files, max_seq_length = datasets.get(dataset_name).values() # selects metric name and paths from above dict\n",
    "pad_to_max_length = True\n",
    "           \n",
    "training_args = TrainingArguments( # huggingface training arguments https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments\n",
    "        output_dir=f\"./runs/{dataset_name}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        max_steps=-1,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        skip_memory_metrics=True,\n",
    "        report_to=\"none\" if os.environ[\"WANDB_DISABLED\"] == \"true\" else \"wandb\",\n",
    "        logging_steps=100, # logging steps for train loss\n",
    "        do_predict=True,\n",
    "        load_best_model_at_end=False, # do we do this here, linkbert doesn't, would require metric_for_best_model and greater_is_better\n",
    "        **trainargs.get(dataset_name)\n",
    "    )\n",
    "\n",
    "### HPO\n",
    "direction=\"maximize\" # maximize if metric is bigger_is_better, else: minimize\n",
    "n_trials = 10 # Number of trials for HPO\n",
    "\n",
    "# Hyperparameter search space, overwriting training_args\n",
    "# see https://docs.ray.io/en/latest/tune/key-concepts.html#search-spaces\n",
    "def hp_space_ray(trial): \n",
    "    return {\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 5e-5),\n",
    "        \"num_train_epochs\": tune.choice(range(10, 30)),\n",
    "        #\"seed\": tune.choice(range(1, 41)), check with set_seed above, needed anyway?\n",
    "        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),\n",
    "    }\n",
    "\n",
    "### Seeding\n",
    "set_seed(training_args.seed) # Set seed before initializing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uneG1gr28N2y"
   },
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hltyIzHhxWP0",
    "outputId": "73fe5de8-56d6-4061-e0b4-870dbd30d6f0"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLBdgg2KajJ9"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "- Load raw dataset from json files\n",
    "- Set `is_regression` (BIOSSES) and `is_multiclass_binary` (HOC)\n",
    "- Create list of labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLmgcSCjainc",
    "outputId": "858387f4-5930-4b05-d8ef-82cefe74dcbe"
   },
   "outputs": [],
   "source": [
    "# Loading a dataset from your local files.\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "is_multiclass_binary = raw_datasets[\"train\"].features[\"label\"].dtype in [\"list\"]\n",
    "\n",
    "if is_regression:\n",
    "    print ('is_regression')\n",
    "    num_labels = 1\n",
    "elif is_multiclass_binary:\n",
    "    print ('is_multiclass_binary')\n",
    "    assert metric_name.startswith(\"hoc\")\n",
    "    num_labels = len(raw_datasets[\"train\"][0][\"label\"])\n",
    "    label_list = list(range(num_labels))\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    print ('\\nlabel_list', label_list)\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "label_to_id = None\n",
    "if not is_regression:\n",
    "  label_to_id = {v: i for i, v in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjaKr2-I-Jq1"
   },
   "source": [
    "## Initialize model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqxxjoTxx63J",
    "outputId": "7ad1f390-e8c4-4baf-ddaa-03889d1fdb36"
   },
   "outputs": [],
   "source": [
    "model_name = \"michiyasunaga/BioLinkBERT-base\"\n",
    "# model_name = \"sshleifer/tiny-distilroberta-base\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "## Needs to be encapsulated for hpo\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config\n",
    "    )\n",
    "    if not is_regression:\n",
    "      model.config.label2id = label_to_id\n",
    "      model.config.id2label = {id: label for label, id in model.config.label2id.items()}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WTSBirP71km"
   },
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Tw7Av3YQ2lI",
    "outputId": "8a262927-0a09-42ad-a2e4-e71bfd3731d6"
   },
   "outputs": [],
   "source": [
    "# Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "    sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "elif \"sentence\" in non_label_column_names:\n",
    "    sentence1_key, sentence2_key = \"sentence\", None\n",
    "else:\n",
    "    if len(non_label_column_names) >= 2:\n",
    "        sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "    else:\n",
    "        sentence1_key, sentence2_key = non_label_column_names[0], None\n",
    "\n",
    "# Padding strategy\n",
    "if pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "\n",
    "if max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({max_seq_length}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        if is_multiclass_binary:\n",
    "            result[\"label\"] = examples[\"label\"]\n",
    "        else:\n",
    "            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "    raw_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "predict_dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-98QinSk0W2R"
   },
   "source": [
    "## Custom evaluation for HOC (https://raw.githubusercontent.com/michiyasunaga/LinkBERT/main/src/seqcls/utils_hoc.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN-2mYjb0KtW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "LABELS = ['activating invasion and metastasis', 'avoiding immune destruction',\n",
    "          'cellular energetics', 'enabling replicative immortality', 'evading growth suppressors',\n",
    "          'genomic instability and mutation', 'inducing angiogenesis', 'resisting cell death',\n",
    "          'sustaining proliferative signaling', 'tumor promoting inflammation']\n",
    "\n",
    "\n",
    "def divide(x, y):\n",
    "    return np.true_divide(x, y, out=np.zeros_like(x, dtype=np.float), where=y != 0)\n",
    "\n",
    "\n",
    "def compute_p_r_f(preds, labels):\n",
    "    TP = ((preds == labels) & (preds != 0)).astype(int).sum()\n",
    "    P_total = (preds != 0).astype(int).sum()\n",
    "    L_total = (labels != 0).astype(int).sum()\n",
    "    P  = divide(TP, P_total).mean()\n",
    "    R  = divide(TP, L_total).mean()\n",
    "    F1 = divide(2 * P * R, (P + R)).mean()\n",
    "    return P, R, F1\n",
    "\n",
    "\n",
    "def eval_hoc(true_list, pred_list, id_list):\n",
    "    data = {}\n",
    "\n",
    "    assert len(true_list) == len(pred_list) == len(id_list), \\\n",
    "        f'Gold line no {len(true_list)} vs Prediction line no {len(pred_list)} vs Id line no {len(id_list)}'\n",
    "\n",
    "    cat = len(LABELS)\n",
    "    assert cat == len(true_list[0]) == len(pred_list[0])\n",
    "\n",
    "    for i in range(len(true_list)):\n",
    "        id = id_list[i]\n",
    "        key = id.split('_')[0]\n",
    "        if key not in data:\n",
    "            data[key] = (set(), set())\n",
    "\n",
    "        for j in range(cat):\n",
    "            if true_list[i][j] == 1:\n",
    "                data[key][0].add(j)\n",
    "            if pred_list[i][j] == 1:\n",
    "                data[key][1].add(j)\n",
    "\n",
    "    print (f\"There are {len(data)} documents in the data set\")\n",
    "    # print ('data', data)\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for k, (true, pred) in data.items():\n",
    "        t = [0] * len(LABELS)\n",
    "        for i in true:\n",
    "            t[i] = 1\n",
    "\n",
    "        p = [0] * len(LABELS)\n",
    "        for i in pred:\n",
    "            p[i] = 1\n",
    "\n",
    "        y_test.append(t)\n",
    "        y_pred.append(p)\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    p, r, f1 = compute_p_r_f(y_pred, y_test)\n",
    "    return {\"precision\": p, \"recall\": r, \"F1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvo24abKzNRR"
   },
   "source": [
    "## Init trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icEs6LWGzJXD",
    "outputId": "5c9a5e81-bc9c-4555-c38e-1f67c6467e1e"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction, eval_dataset):\n",
    "    \n",
    "    metric = load_metric(\"accuracy\")\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    if metric_name == \"hoc\":\n",
    "        labels = np.array(p.label_ids).astype(int) #[num_ex, num_class]\n",
    "        preds = (np.array(preds) > 0).astype(int)  #[num_ex, num_class]\n",
    "        ids = eval_dataset[\"id\"]\n",
    "        return eval_hoc(labels.tolist(), preds.tolist(), list(ids))\n",
    "\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    \n",
    "    if metric_name == \"pearsonr\":\n",
    "        from scipy.stats import pearsonr as scipy_pearsonr\n",
    "        pearsonr = float(scipy_pearsonr(p.label_ids, preds)[0])\n",
    "        return {\"pearsonr\": pearsonr}\n",
    "    elif metric_name == \"PRF1\":\n",
    "        TP = ((preds == p.label_ids) & (preds != 0)).astype(int).sum().item()\n",
    "        P_total = (preds != 0).astype(int).sum().item()\n",
    "        L_total = (p.label_ids != 0).astype(int).sum().item()\n",
    "        P = TP / P_total if P_total else 0\n",
    "        R = TP / L_total if L_total else 0\n",
    "        F1 = 2 * P * R / (P + R) if (P + R) else 0\n",
    "        return {\"precision\": P, \"recall\": R, \"F1\": F1}\n",
    "    elif is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "if pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif training_args.fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = SeqClsTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-be15qz6FWM"
   },
   "source": [
    "# Train/Eval/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkMPnZzn6Bhg"
   },
   "source": [
    "## With HPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjCbxn7D3R29",
    "outputId": "e6bd17c8-79fa-4604-9b60-78db04131699"
   },
   "outputs": [],
   "source": [
    "## needed only for google colab\n",
    "# ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
    "\n",
    "reporter = JupyterNotebookReporter(False)\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "  direction=direction,\n",
    "  backend=\"ray\",\n",
    "  hp_space=hp_space_ray,\n",
    "  keep_checkpoints_num=1,\n",
    "  n_trials=n_trials,\n",
    "  local_dir=f\"./runs/{dataset_name}/ray_results/\",\n",
    "  name=dataset_name,\n",
    "  progress_reporter=reporter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytKxOvhwZfxB"
   },
   "source": [
    "### Load best model from HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvSuAGBGZgIG"
   },
   "outputs": [],
   "source": [
    "def recover_checkpoint(tune_checkpoint_dir, model_name=None):\n",
    "    if tune_checkpoint_dir is None or len(tune_checkpoint_dir) == 0:\n",
    "        return model_name\n",
    "    # Get subdirectory used for Huggingface.\n",
    "    subdirs = [\n",
    "        os.path.join(tune_checkpoint_dir, name)\n",
    "        for name in os.listdir(tune_checkpoint_dir)\n",
    "        if os.path.isdir(os.path.join(tune_checkpoint_dir, name))\n",
    "    ]\n",
    "    # There should only be 1 subdir.\n",
    "    assert len(subdirs) == 1, subdirs\n",
    "    return subdirs[0]\n",
    "\n",
    "ray_result_dir = f\"./runs/{dataset_name}/ray_results/{dataset_name}\"\n",
    "\n",
    "from ray.tune import ExperimentAnalysis\n",
    "analysis = ExperimentAnalysis(ray_result_dir)\n",
    "best_checkpoint = recover_checkpoint(\n",
    "    analysis.get_best_trial(metric=\"objective\",\n",
    "                            mode=\"max\" if direction==\"maximize\" else \"min\").checkpoint.value\n",
    ")\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    best_checkpoint)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = SeqClsTrainer(\n",
    "    model=best_model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CSbl8_wZjl4"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "FmPnL-KYQX68",
    "outputId": "1028842f-7682-46e7-f838-b1f9a46c7f6d"
   },
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZaoPeMNZX2O"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "7aOt8cXCZXQN",
    "outputId": "47089b96-a97a-424a-a037-42ef5280c490"
   },
   "outputs": [],
   "source": [
    "results = trainer.predict(predict_dataset, metric_key_prefix=\"test\")\n",
    "predictions = results.predictions\n",
    "metrics = results.metrics\n",
    "metrics[\"test_samples\"] = len(predict_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)\n",
    "trainer.log(metrics)\n",
    "\n",
    "import json\n",
    "output_dir = training_args.output_dir\n",
    "output_path = f\"{output_dir}/test_outputs.json\"\n",
    "json.dump({\"predictions\": results.predictions.tolist(), \"label_ids\": results.label_ids.tolist()},\n",
    "              open(output_path, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hw8den-YahQ"
   },
   "source": [
    "## Simple (should not be used/only for reprocucing LinkBERT numbers)\n",
    "\n",
    "To reproduce exact numbers\n",
    "\n",
    "- do `model=model_init()` right after model_init function definition (in \"Initialize model, tokenizer, config\") and change in Trainer init `model_init=model_init` to `model=model` (Otherwise the rng is not the same with the original script)\n",
    "- Install same library versions as LinkBERT\n",
    "\n",
    "```\n",
    "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install transformers==4.9.1 datasets==1.11.0 fairscale==0.4.0 wandb sklearn seqeval\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7oouukJzAru"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "KJukRskTzBDl",
    "outputId": "a4696754-b652-431a-ccb9-79b55826891f"
   },
   "outputs": [],
   "source": [
    "## To get exacly the same numbers, do `model=model_init()` right after model_init function definition (in \"Initialize model, tokenizer, config\")\n",
    "## And change in Trainer init `model_init=model_init` to `model=model` (Otherwise the rng is not the same with the original script)\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(train_dataset)\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg9dQFUGy-Lg"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "ToTiJ6nHy61B",
    "outputId": "3b05c4bb-60db-473c-9eca-7a694bc12a37"
   },
   "outputs": [],
   "source": [
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EywNFTMwy4gu"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "z6NHb4rNy2R-",
    "outputId": "094ce51a-5bb2-4ba0-9d1c-de04426b7291"
   },
   "outputs": [],
   "source": [
    "logger.info(\"*** Predict ***\")\n",
    "\n",
    "results = trainer.predict(predict_dataset, metric_key_prefix=\"test\")\n",
    "predictions = results.predictions\n",
    "metrics = results.metrics\n",
    "metrics[\"test_samples\"] = len(predict_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)\n",
    "trainer.log(metrics)\n",
    "\n",
    "import json\n",
    "output_dir = training_args.output_dir\n",
    "output_path = f\"{output_dir}/test_outputs.json\"\n",
    "json.dump({\"predictions\": results.predictions.tolist(), \"label_ids\": results.label_ids.tolist()},\n",
    "              open(output_path, \"w\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Run_seqcls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

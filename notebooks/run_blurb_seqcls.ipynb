{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DSm8ui9tI39"
      },
      "source": [
        "# Notebook for all seqcls (PubMedQA, BioASQ, BIOSSES, HOC, ChemProt, DDI, GAD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Prerequisites\n",
        "\n",
        "+ Open the notebooks for [seqcls datasets](https://colab.research.google.com/drive/18En1Y-OB6_sv3aik7L7iLMN7k41uExbu?usp=sharing) (PubMedQA, BioASQ, Biosses, hoc, ChemProt, DDI, GAD) or [tokcls datasets](https://colab.research.google.com/drive/1XTa3NJtQ5lkveec3WaniJM-0we6gZxtl?usp=sharing) (ebmnlp, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem)\n",
        "+ Copy the notebook to your drive (File -> Copy to Drive)\n",
        "\n",
        "### Instructions\n",
        "\n",
        "**Note: The only cell which requires editing should be the \"Configuration\" cell.**\n",
        "\n",
        "The notebook contains 4 major segments:\n",
        "\n",
        "1. Preparation: Contains pretty self-explanatory cells \"Download BLURB data\", \"Install libraries\" and \"Import dependencies, class definitions, static settings\". Should be run only once in the best case.\n",
        "2. Configuration: This cell should be the only one that should be edited. The most important variables are:\n",
        "\n",
        "  * `dataset_name`: should be one of `[PubMedQA, BioASQ, Biosses, hoc, ChemProt, DDI, GAD]` for seqcls or one of `[ebmnlp, JNLPBA, NCBI-disease, BC2GM, BC5CDR-disease, BC5CDR-chem]` for tokcls\n",
        "  * `max_train_epochs`: The maximum number of training epochs.\n",
        "  * `n_trials`: The number of trials in the hyperparameter search\n",
        "  * `hp_space_ray`: The hyperparameter search space\n",
        "3. Train with hyperparameter optimization: Runs the hyperparameter optimization with the configuration specified by `n_trials` and `hp_space_ray` in the configuration. The experiment should be located in `/runs/{dataset_name}/ray_results/{dataset_name}` where `dataset_name` is the dataset name that you specified.\n",
        "4. Load and evaluate best model from hyperparameter optimization: selects the best checkpoint from the experiment folder and evaluates the test set."
      ],
      "metadata": {
        "id": "32-JDnJSLUdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "fdfSczbxLayj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31vu0gY6SxU"
      },
      "source": [
        "### Download BLURB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22IX9SdKrhU3"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYWaY4Aw6Wde"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mLo-nowroW9"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers datasets fairscale sklearn seqeval\n",
        "!pip install ray psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcciYtpq6adU"
      },
      "source": [
        "### Import dependencies, class definitions, static settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0LRohsLtst4"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, is_torch_tpu_available\n",
        "from transformers.trainer_utils import PredictionOutput\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import psutil\n",
        "import random\n",
        "import sys\n",
        "import uuid\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import JupyterNotebookReporter\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "# Disable Wandb, set local rank to -1\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"-1\"\n",
        "\n",
        "# Define SeqClsTrainer\n",
        "# Copied straight from https://raw.githubusercontent.com/michiyasunaga/LinkBERT/main/src/seqcls/trainer_seqcls.py\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "A subclass of `Trainer` specific to Question-Answering tasks\n",
        "\"\"\"\n",
        "\n",
        "class SeqClsTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
        "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "\n",
        "        # Temporarily disable metric computation, we will do it in the loop here.\n",
        "        compute_metrics = self.compute_metrics\n",
        "        self.compute_metrics = None\n",
        "        eval_loop = self.evaluation_loop\n",
        "        output = eval_loop(\n",
        "                eval_dataloader,\n",
        "                description=\"Evaluation\",\n",
        "                prediction_loss_only=None,\n",
        "                ignore_keys=ignore_keys,\n",
        "        )\n",
        "        # self.label_names = label_names\n",
        "        self.compute_metrics = compute_metrics\n",
        "\n",
        "        # metrics = output.metrics\n",
        "        metrics = self.compute_metrics(output, eval_dataset)\n",
        "        metrics[f\"{metric_key_prefix}_loss\"] = output.metrics[f\"{metric_key_prefix}_loss\"]\n",
        "\n",
        "        # Prefix all keys with metric_key_prefix + '_'\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
        "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        self.log(metrics)\n",
        "\n",
        "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
        "        return metrics\n",
        "\n",
        "    def predict(self, predict_dataset, ignore_keys=None, metric_key_prefix: str = \"test\"):\n",
        "        predict_dataloader = self.get_test_dataloader(predict_dataset)\n",
        "\n",
        "        # Temporarily disable metric computation, we will do it in the loop here.\n",
        "        compute_metrics = self.compute_metrics\n",
        "        self.compute_metrics = None\n",
        "        eval_loop = self.evaluation_loop\n",
        "        output = eval_loop(\n",
        "            predict_dataloader,\n",
        "            description=\"Prediction\",\n",
        "            prediction_loss_only=None,\n",
        "            ignore_keys=ignore_keys,\n",
        "        )\n",
        "\n",
        "        # self.label_names = label_names\n",
        "        self.compute_metrics = compute_metrics\n",
        "\n",
        "        # metrics = output.metrics\n",
        "        metrics = self.compute_metrics(output, predict_dataset)\n",
        "\n",
        "        # Prefix all keys with metric_key_prefix + '_'\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
        "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        self.log(metrics) #Added\n",
        "\n",
        "        return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=metrics)\n",
        "\n",
        "### Paths and metric names of datasets\n",
        "datasets = {\n",
        "    \"PubMedQA\": {\n",
        "        \"metric_name\": \"accuracy\",\n",
        "        \"metric_for_best_model\": \"accuracy\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/pubmedqa_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/pubmedqa_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/pubmedqa_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 512,\n",
        "    },\n",
        "    \"BioASQ\": {\n",
        "        \"metric_name\": \"accuracy\",\n",
        "        \"metric_for_best_model\": \"accuracy\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/bioasq_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/bioasq_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/bioasq_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 512,\n",
        "    },\n",
        "    \"Biosses\": {\n",
        "        \"metric_name\": \"pearsonr\",\n",
        "        \"metric_for_best_model\": \"pearsonr\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/BIOSSES_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/BIOSSES_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/BIOSSES_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 512,\n",
        "    },\n",
        "    \"hoc\": {\n",
        "        \"metric_name\": \"hoc\",\n",
        "        \"metric_for_best_model\": \"F1\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/HoC_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/HoC_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/HoC_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 512,\n",
        "    },\n",
        "    \"ChemProt\": {\n",
        "        \"metric_name\": \"PRF1\",\n",
        "        \"metric_for_best_model\": \"F1\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/chemprot_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/chemprot_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/chemprot_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 256,\n",
        "    },\n",
        "    \"DDI\": {\n",
        "        \"metric_name\": \"PRF1\",\n",
        "        \"metric_for_best_model\": \"F1\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/DDI_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/DDI_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/DDI_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 256,\n",
        "    },\n",
        "    \"GAD\": {\n",
        "        \"metric_name\": \"PRF1\",\n",
        "        \"metric_for_best_model\": \"F1\",\n",
        "        \"data_files\": {\n",
        "          \"train\": \"data/seqcls/GAD_hf/train.json\", \n",
        "          \"validation\": \"data/seqcls/GAD_hf/dev.json\", \n",
        "          \"test\": \"data/seqcls/GAD_hf/test.json\"\n",
        "        },\n",
        "        \"max_seq_length\": 256,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gvweJclwHaJ"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmk7wJFF00C3"
      },
      "outputs": [],
      "source": [
        "# General model/dataset configuration\n",
        "\n",
        "## Name of the model to load, shouldn't be changed\n",
        "model_name = \"leonweber/bunsen_base_last\" \n",
        "\n",
        "## Name of the dataset to finetune, one of [PubMedQA, BioASQ, Biosses, hoc, ChemProt, DDI, GAD]\n",
        "dataset_name = \"PubMedQA\" \n",
        "\n",
        "## Automatically selects metric name and paths from above dict\n",
        "(metric_name, \n",
        " metric_for_best_model, \n",
        " data_files, \n",
        " max_seq_length) = datasets.get(dataset_name).values()\n",
        "\n",
        "## Always true, should not be changed\n",
        "pad_to_max_length = True\n",
        "\n",
        "## The number of maximum training epochs\n",
        "## Every epoch a checkpoint is created, after training the one with the highest `metric_for_best_model` is selected.\n",
        "max_train_epochs=30\n",
        "           \n",
        "## Huggingface training arguments \n",
        "## see https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments\n",
        "training_args = TrainingArguments( \n",
        "        output_dir=f\"./runs/{dataset_name}\",\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        per_device_eval_batch_size=8,\n",
        "        logging_dir=\"./logs\",\n",
        "        skip_memory_metrics=True,\n",
        "        report_to=\"none\",\n",
        "        logging_steps=100, # logging steps for train loss\n",
        "        do_predict=True,\n",
        "        load_best_model_at_end=True, # do we do this here, linkbert doesn't, would require metric_for_best_model and greater_is_better\n",
        "        metric_for_best_model=metric_for_best_model,\n",
        "        greater_is_better=True,\n",
        "        gradient_accumulation_steps=1,\n",
        "        fp16=True,\n",
        "        num_train_epochs=max_train_epochs\n",
        "    )\n",
        "\n",
        "# Hyperparameter optimization (HPO)\n",
        "\n",
        "# maximize if metric is bigger_is_better, else: minimize, should not be changed\n",
        "direction=\"maximize\" \n",
        "\n",
        "## Number of different trials for HPO\n",
        "## Note that if you have grid_search (e.g. tune.choice([4, 8, 16])) in the config below, `n_trials` is how many\n",
        "## trials (samples of random variables) are tried for each distinct combination\n",
        "## of grid search. F.e. n_trails=2 and the following config\n",
        "##\n",
        "## hp_space_ray(trial): \n",
        "##    return {\n",
        "##        \"per_device_train_batch_size\": tune.grid_search([4, 8, 16]),\n",
        "##        \"num_train_epochs\": tune.grid_search([5,10,20]),\n",
        "##        \"learning_rate\": tune.loguniform(1e-5, 5e-5),\n",
        "##    }\n",
        "##\n",
        "## would result in a total of 18 trials.\n",
        "## see https://docs.ray.io/en/latest/tune/api_docs/search_space.html#\n",
        "n_trials = 10 \n",
        "\n",
        "## Hyperparameter search space, \n",
        "## These overwrite training_args above\n",
        "## For forther information see see https://docs.ray.io/en/latest/tune/key-concepts.html#search-spaces\n",
        "## This should not contain configurations for the parameter 'num_train_epochs' see variable 'max_train_epochs' above\n",
        "def hp_space_ray(trial): \n",
        "    return {\n",
        "        \"learning_rate\": tune.loguniform(1e-5, 5e-5),\n",
        "        \"per_device_train_batch_size\": tune.choice([4, 8, 16]),\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-be15qz6FWM"
      },
      "source": [
        "# Train with hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLmgcSCjainc"
      },
      "outputs": [],
      "source": [
        "### Seeding\n",
        "set_seed(training_args.seed) # Set seed before everything\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")\n",
        "\n",
        "log_level = training_args.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "# Log on each process the small summary:\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")\n",
        "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "# Loading a dataset from your local files.\n",
        "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
        "# Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
        "is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
        "is_multiclass_binary = raw_datasets[\"train\"].features[\"label\"].dtype in [\"list\"]\n",
        "\n",
        "if is_regression:\n",
        "    print ('is_regression')\n",
        "    num_labels = 1\n",
        "elif is_multiclass_binary:\n",
        "    print ('is_multiclass_binary')\n",
        "    assert metric_name.startswith(\"hoc\")\n",
        "    num_labels = len(raw_datasets[\"train\"][0][\"label\"])\n",
        "    label_list = list(range(num_labels))\n",
        "else:\n",
        "    # A useful fast method:\n",
        "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
        "    label_list = raw_datasets[\"train\"].unique(\"label\")\n",
        "    label_list.sort()  # Let's sort it for determinism\n",
        "    print ('\\nlabel_list', label_list)\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "label_to_id = None\n",
        "if not is_regression:\n",
        "  label_to_id = {v: i for i, v in enumerate(label_list)}\n",
        "\n",
        "# Initialize model, tokenizer, config\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "## Needs to be encapsulated for hpo\n",
        "def model_init():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        config=config\n",
        "    )\n",
        "    if not is_regression:\n",
        "      model.config.label2id = label_to_id\n",
        "      model.config.id2label = {id: label for label, id in model.config.label2id.items()}\n",
        "    return model\n",
        "\n",
        "# Preprocess dataset\n",
        "# Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
        "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
        "if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
        "    sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
        "elif \"sentence\" in non_label_column_names:\n",
        "    sentence1_key, sentence2_key = \"sentence\", None\n",
        "else:\n",
        "    if len(non_label_column_names) >= 2:\n",
        "        sentence1_key, sentence2_key = non_label_column_names[:2]\n",
        "    else:\n",
        "        sentence1_key, sentence2_key = non_label_column_names[0], None\n",
        "\n",
        "# Padding strategy\n",
        "if pad_to_max_length:\n",
        "    padding = \"max_length\"\n",
        "else:\n",
        "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
        "    padding = False\n",
        "\n",
        "if max_seq_length > tokenizer.model_max_length:\n",
        "    logger.warning(\n",
        "        f\"The max_seq_length passed ({max_seq_length}) is larger than the maximum length for the\"\n",
        "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "    )\n",
        "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the texts\n",
        "    args = (\n",
        "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "    )\n",
        "\n",
        "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "    # Map labels to IDs (not necessary for GLUE tasks)\n",
        "    if label_to_id is not None and \"label\" in examples:\n",
        "        if is_multiclass_binary:\n",
        "            result[\"label\"] = examples[\"label\"]\n",
        "        else:\n",
        "            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
        "    return result\n",
        "\n",
        "with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
        "    raw_datasets = raw_datasets.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        desc=\"Running tokenizer on dataset\",\n",
        "    )\n",
        "\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "predict_dataset = raw_datasets[\"test\"]\n",
        "\n",
        "# Custom evaluation for HOC (https://raw.githubusercontent.com/michiyasunaga/LinkBERT/main/src/seqcls/utils_hoc.py)\n",
        "import numpy as np\n",
        "\n",
        "LABELS = ['activating invasion and metastasis', 'avoiding immune destruction',\n",
        "          'cellular energetics', 'enabling replicative immortality', 'evading growth suppressors',\n",
        "          'genomic instability and mutation', 'inducing angiogenesis', 'resisting cell death',\n",
        "          'sustaining proliferative signaling', 'tumor promoting inflammation']\n",
        "\n",
        "\n",
        "def divide(x, y):\n",
        "    return np.true_divide(x, y, out=np.zeros_like(x, dtype=np.float), where=y != 0)\n",
        "\n",
        "\n",
        "def compute_p_r_f(preds, labels):\n",
        "    TP = ((preds == labels) & (preds != 0)).astype(int).sum()\n",
        "    P_total = (preds != 0).astype(int).sum()\n",
        "    L_total = (labels != 0).astype(int).sum()\n",
        "    P  = divide(TP, P_total).mean()\n",
        "    R  = divide(TP, L_total).mean()\n",
        "    F1 = divide(2 * P * R, (P + R)).mean()\n",
        "    return P, R, F1\n",
        "\n",
        "\n",
        "def eval_hoc(true_list, pred_list, id_list):\n",
        "    data = {}\n",
        "\n",
        "    assert len(true_list) == len(pred_list) == len(id_list), \\\n",
        "        f'Gold line no {len(true_list)} vs Prediction line no {len(pred_list)} vs Id line no {len(id_list)}'\n",
        "\n",
        "    cat = len(LABELS)\n",
        "    assert cat == len(true_list[0]) == len(pred_list[0])\n",
        "\n",
        "    for i in range(len(true_list)):\n",
        "        id = id_list[i]\n",
        "        key = id.split('_')[0]\n",
        "        if key not in data:\n",
        "            data[key] = (set(), set())\n",
        "\n",
        "        for j in range(cat):\n",
        "            if true_list[i][j] == 1:\n",
        "                data[key][0].add(j)\n",
        "            if pred_list[i][j] == 1:\n",
        "                data[key][1].add(j)\n",
        "\n",
        "    print (f\"There are {len(data)} documents in the data set\")\n",
        "    # print ('data', data)\n",
        "\n",
        "    y_test = []\n",
        "    y_pred = []\n",
        "    for k, (true, pred) in data.items():\n",
        "        t = [0] * len(LABELS)\n",
        "        for i in true:\n",
        "            t[i] = 1\n",
        "\n",
        "        p = [0] * len(LABELS)\n",
        "        for i in pred:\n",
        "            p[i] = 1\n",
        "\n",
        "        y_test.append(t)\n",
        "        y_pred.append(p)\n",
        "\n",
        "    y_test = np.array(y_test)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    p, r, f1 = compute_p_r_f(y_pred, y_test)\n",
        "    return {\"precision\": p, \"recall\": r, \"F1\": f1}\n",
        "\n",
        "def compute_metrics(p: EvalPrediction, eval_dataset):\n",
        "    \n",
        "    metric = load_metric(\"accuracy\")\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    if metric_name == \"hoc\":\n",
        "        labels = np.array(p.label_ids).astype(int) #[num_ex, num_class]\n",
        "        preds = (np.array(preds) > 0).astype(int)  #[num_ex, num_class]\n",
        "        ids = eval_dataset[\"id\"]\n",
        "        return eval_hoc(labels.tolist(), preds.tolist(), list(ids))\n",
        "\n",
        "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
        "    \n",
        "    if metric_name == \"pearsonr\":\n",
        "        from scipy.stats import pearsonr as scipy_pearsonr\n",
        "        pearsonr = float(scipy_pearsonr(p.label_ids, preds)[0])\n",
        "        return {\"pearsonr\": pearsonr}\n",
        "    elif metric_name == \"PRF1\":\n",
        "        TP = ((preds == p.label_ids) & (preds != 0)).astype(int).sum().item()\n",
        "        P_total = (preds != 0).astype(int).sum().item()\n",
        "        L_total = (p.label_ids != 0).astype(int).sum().item()\n",
        "        P = TP / P_total if P_total else 0\n",
        "        R = TP / L_total if L_total else 0\n",
        "        F1 = 2 * P * R / (P + R) if (P + R) else 0\n",
        "        return {\"precision\": P, \"recall\": R, \"F1\": F1}\n",
        "    elif is_regression:\n",
        "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
        "    else:\n",
        "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
        "\n",
        "# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
        "if pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "elif training_args.fp16:\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "else:\n",
        "    data_collator = None\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = SeqClsTrainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "## needed only for google colab\n",
        "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total\n",
        "\n",
        "metric_columns = [\"epoch\", \"training_iteration\", \"objective\", \"eval_loss\"]\n",
        "if metric_name == \"pearsonr\":\n",
        "  metric_columns.extend([\"eval_pearsonr\"])\n",
        "elif metric_name == \"PRF1\" or metric_name == \"hoc\":\n",
        "  metric_columns.extend([\"eval_precision\", \"eval_recall\", \"eval_F1\"])\n",
        "else:\n",
        "  metric_columns.extend([\"eval_accuracy\"])\n",
        "\n",
        "\n",
        "reporter = JupyterNotebookReporter(\n",
        "    False,\n",
        "    metric_columns=metric_columns,\n",
        ")\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "  direction=direction,\n",
        "  backend=\"ray\",\n",
        "  hp_space=hp_space_ray,\n",
        "  keep_checkpoints_num=1,\n",
        "  checkpoint_score_attr=\"objective\",\n",
        "  n_trials=n_trials,\n",
        "  local_dir=f\"./runs/{dataset_name}/ray_results/\",\n",
        "  name=dataset_name,\n",
        "  progress_reporter=reporter,\n",
        "  resources_per_trial={\n",
        "      \"cpu\": 1,\n",
        "      \"gpu\": 1\n",
        "  },\n",
        "  compute_objective = lambda x: x[\"eval_\" + metric_for_best_model]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytKxOvhwZfxB"
      },
      "source": [
        "# Load and evaluate best model from HPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvSuAGBGZgIG"
      },
      "outputs": [],
      "source": [
        "def recover_checkpoint(tune_checkpoint_dir, model_name=None):\n",
        "    if tune_checkpoint_dir is None or len(tune_checkpoint_dir) == 0:\n",
        "        return model_name\n",
        "    # Get subdirectory used for Huggingface.\n",
        "    subdirs = [\n",
        "        os.path.join(tune_checkpoint_dir, name)\n",
        "        for name in os.listdir(tune_checkpoint_dir)\n",
        "        if os.path.isdir(os.path.join(tune_checkpoint_dir, name))\n",
        "    ]\n",
        "    # There should only be 1 subdir.\n",
        "    assert len(subdirs) == 1, subdirs\n",
        "    return subdirs[0]\n",
        "\n",
        "ray_result_dir = f\"./runs/{dataset_name}/ray_results/{dataset_name}\"\n",
        "\n",
        "from ray.tune import ExperimentAnalysis\n",
        "analysis = ExperimentAnalysis(ray_result_dir)\n",
        "best_checkpoint = recover_checkpoint(\n",
        "    analysis.get_best_checkpoint(analysis.get_best_trial(metric=\"objective\",\n",
        "                            mode=\"max\" if direction==\"maximize\" else \"min\"), metric=\"objective\",\n",
        "                            mode=\"max\" if direction==\"maximize\" else \"min\").local_path\n",
        ")\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    best_checkpoint)\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = SeqClsTrainer(\n",
        "    model=best_model,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "metrics[\"eval_samples\"] = len(eval_dataset)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "# Predict\n",
        "results = trainer.predict(predict_dataset, metric_key_prefix=\"test\")\n",
        "predictions = results.predictions\n",
        "metrics = results.metrics\n",
        "metrics[\"test_samples\"] = len(predict_dataset)\n",
        "\n",
        "trainer.log_metrics(\"test\", metrics)\n",
        "trainer.save_metrics(\"test\", metrics)\n",
        "trainer.log(metrics)\n",
        "\n",
        "import json\n",
        "output_dir = training_args.output_dir\n",
        "output_path = f\"{output_dir}/test_outputs.json\"\n",
        "json.dump({\"predictions\": results.predictions.tolist(), \"label_ids\": results.label_ids.tolist()},\n",
        "              open(output_path, \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AGqNt41cXWaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "run_blurb_seqcls.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
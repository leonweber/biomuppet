local transformer_model = "bert-base-multilingual-cased";
local transformer_dim = 768;

//local transformer_model = "xlm-roberta-large";
//local transformer_dim = 1024;

local max_len = 128;

{
    "random_seed": 8446,
    "numpy_seed": 8446,
    "pytorch_seed": 8446,
    "dataset_reader": {
        "type": "machamp_universal_reader",
        "target_max_tokens": max_len,
        "source_max_tokens": max_len,
        "do_lowercase": false,

        "token_indexers": {
            "tokens": {
                "type": "pretrained_transformer_mixmatched",
                "max_length": max_len,
                "model_name": transformer_model
            }
        },
        "tokenizer": {
            "type": "pretrained_transformer",
            "add_special_tokens": false,
            "model_name": transformer_model,
        },
        "target_token_indexers": {
            "tokens": {
                "namespace": "target_words"
            }
        },
        "target_tokenizer":{
             "type": "bert_basic_tokenizer"
        }
    },
    "vocabulary": {
        "max_vocab_size": {"target_words": 50000},
        "min_count": {
            "source_words": 1,
            "target_words": 1
        }
    },
    "model": {
        "type": "machamp_model",
        "dataset_embeds_dim": 0,
        "decoders": {
            "classification": {
                "type": "machamp_sentence_decoder"
            },
            "default": {
                "input_dim": transformer_dim,
                "loss_weight": 1,
                "order": 1
            },
            "dependency": {
                "type": "machamp_dependency_decoder",
                "arc_representation_dim": transformer_dim,
                "tag_representation_dim": 256,
                "use_mst_decoding_for_validation": true
            },
            "multiseq": {
                "type": "machamp_multiseq_decoder",
                "metric": "multi_span_f1"
            },
            "seq": {
                "type": "machamp_tag_decoder"
            },
            "seq2seq": {
                "type": "machamp_seq2seq_decoder",
                "attention": "dot_product",
                "beam_size": 6,
                "max_decoding_steps": 128,
                "target_decoder_layers": 2,
                "target_embedding_dim": 512,
                "use_bleu": true
            },
            "seq_bio": {
                "type": "machamp_crf_decoder",
                "metric": "span_f1"
            },
            "string2string": {
                "type": "machamp_tag_decoder"
            },
            "mlm": {
                "type": "machamp_mlm_decoder",
                "pretrained_model": transformer_model
            }
        },
        "default_max_sents": 0,
        "dropout": 0.3,
        "encoder": {
            "type": "cls_pooler",
            "cls_is_last_token": false,
            "embedding_dim": transformer_dim
        },
        "text_field_embedder": {
            "type": "basic",
            "token_embedders": {
                "tokens": {
                    "type": "machamp_pretrained_transformer_mismatched",
                    "layers_to_use": [-1],
                    "max_length": max_len,
                    "model_name": transformer_model,
                    "train_parameters": true
                }
            }
        }
    },
    "data_loader": {
        "batch_sampler": {
            "type": "dataset_buckets",
            "max_tokens": 1024,
            "batch_size": 32,
            "sampling_smoothing": 1.0, //1.0 == original size
            "sorting_keys": [
                "tokens"
            ]
        }
    },
    "trainer": {
        "checkpointer": {
            "num_serialized_models_to_keep": 1
        },
        "use_amp": false, // could save some memory on gpu
        "grad_norm": 1,
        "learning_rate_scheduler": {
            "type": "polynomial_decay",
            "warmup_steps": 50000,
        },
        "num_epochs": 150,
        "optimizer": {
            "type": "huggingface_adamw",
            "betas": [0.9, 0.99],
            "correct_bias": false,
            "lr": 0.0001,
            "parameter_groups": [
                [
                    [
                        "^_text_field_embedder.*"
                    ],
                    {}
                ],
                [
                    [
                        "^decoders.*",
                        "dataset_embedder.*"
                    ],
                    {}
                ]
            ],
            "weight_decay": 0.01
        },
        //"patience": 5, // disabled, because slanted_triangular changes the lr dynamically
        "validation_metric": "+.run/.sum"
    },
    "distributed": {
    "cuda_devices": [0, 1, 2, 3],
    },
    "datasets_for_vocab_creation": [
        "train",
        "validation"//TODO can this be removed now that we add padding/unknown?
    ]
}

